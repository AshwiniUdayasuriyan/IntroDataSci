---
title: "TreeMethods"
author: "CMSC320"
date: "April 7, 2016"
output: html_document
---

```{r, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

We saw in previous units the limitation of using linear methods for classification. In particular, the partition of predictor space into regions according to conditional class probabilities is very limited. In this unit, we look at a set of elegant and versatile methods that allow these regions to take more complex shapes, but still produce models that are interpretable. These are very popular, well-known and studied methods in Statistical Learning. We will concentrate on Regression and Decision Trees and their extension to Random Forests.

### Regression Trees

Consider the dataset we saw in our linear regression unit. We found using linear regression that a linear model of weight vs. miles per gallon was not a good fit. 

```{r, echo=FALSE}
library(tree)
library(ISLR)
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))
data(Auto)


with(Auto, plot(weight, mpg, pch=19, cex=1.4))
legend("topright", pch=19, col=1:3, legend=1:3)
```

Let's take a look at what a regression tree estimates in this case.

```{r}
tree <- tree(mpg~weight, data=Auto)
plot(tree)
text(tree, pretty=0, cex=1.3)
```

The decision trees partitions the `weight` predictor into regions based on its value. We can show this graphically as below. The idea behind the regression tree is that outcome $Y$ (`mpg` in this case) is estimated (or predicted) to be it's mean _within each of the data partitions_. Think of it as the conditional mean of $Y$ where conditioning is given by this region partitioning.

```{r, echo=FALSE, cache=FALSE, results="hide"}
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))

with(Auto, plot(weight, mpg, pch=19, cex=1.4))
#abline(h=subset(tree$frame, grepl("leaf", tree$frame$var))$yval)
abline(v=as.numeric(gsub("<", "", subset(tree$frame, !grepl("leaf", tree$frame$var))$splits[,"cutleft"])))

process_node <- function(i, left, right) {
 if (tree$frame$var[i] == "<leaf>") {
   val <- as.numeric(tree$frame$yval[i])
   segments(left, val, right, val, col="red", lwd=5)
 } else {
   val <- as.numeric(gsub("<","",tree$frame$splits[i, "cutleft"]))
   i <- process_node(i+1, left, val)
   i <- process_node(i+1, val, right)
 }
 i
}

process_node(1, .85*min(Auto$weight), 1.05*max(Auto$weight))
```

Regression and decision trees operate by prediction an outcome variable $Y$ by partitioning feature (predictor) space.

The regression tree model then:

1. Partitions space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, predict response as mean of response for training observations in $R_j$.

The important observation is that **Regression Trees create partition recursively**


For example, consider finding a good predictor $j$ to partition space its axis. A recursive algorithm would look like this:

1. Find predictor $j$ and value $s$ that minimize RSS:

$$
\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2
$$

Where $R_1$ and $R_2$ are regions resulting from splitting observations on predictor $j$ and value $s$:

$$
R_1(j,s) = \{X|X_j < s\} \mathrm{ and } R_2(j,s) \{X|X_j \geq s\}
$$

This is then applied recursively to regions $R_1$ and $R_2$. Within each region a prediction is made using $\hat{y}_{R_j}$ which is the mean of the response $Y$ of observations in $R_j$.

![](8.3.png)

Consider building a model that used both `horsepower` and `weight`. In this plot the value of the response $Y$ is indicated by the size of the point.

```{r, echo=FALSE}
with(Auto, {
     plot(horsepower, weight, cex=mpg/median(mpg), pch=19)

    qs <- quantile(mpg, p=seq(0,1, len=5))
    legend("bottomright", pch=19, legend=qs, pt.cex=qs/median(mpg))
})
```

This is what a decision tree would look like for these two predictors:

```{r}
tree <- tree(mpg~horsepower+weight, data=Auto)
plot(tree)
text(tree, pretty=0)
```

```{r, echo=FALSE, cache=FALSE}
process_node <- function(i, j, left, right, bottom, top, dat) {
  var <- as.character(tree$frame$var[i])
  is_leaf <- grepl("leaf", var)
  
  if (is_leaf) {
    val <- as.numeric(tree$frame$yval[i])
    dat[j,] <- c(j, left, right, bottom, top, val)
    j <- j + 1
  } else {
    val <- as.numeric(gsub("<","",tree$frame$splits[i, "cutleft"]))
    if (var == "horsepower") {
      res <- process_node(i+1, j, left, val, bottom, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
      res <- process_node(i+1, j, val, right, bottom, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
    } else {
      res <- process_node(i+1, j, left, right, bottom, val, dat)
      i <- res$i; j <- res$j; dat <- res$dat
      res <- process_node(i+1, j, left, right, val, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
    }
  }
  list(i=i, j=j, dat=dat)
}

nleaves <- sum(grepl("leaf", tree$frame$var))
region_dat <- data.frame(j=integer(nleaves),
                  left=numeric(nleaves),
                  right=numeric(nleaves),
                  bottom=numeric(nleaves),
                  top=numeric(nleaves),
                  val=numeric(nleaves))

res <- process_node(1, 1, .85*min(Auto$horsepower), 1.05*max(Auto$horsepower), .85*min(Auto$weight), 1.05*max(Auto$weight), region_dat)
region_dat <- res$dat

with(Auto, {
     plot(horsepower, weight, cex=mpg/median(mpg), pch=19)

    qs <- quantile(mpg, p=seq(0,1, len=5))
    legend("bottomright", pch=19, legend=qs, pt.cex=qs/median(mpg))
})

with(region_dat, {
  segments(left, bottom, right, bottom)
  segments(left, top, right, top)
  segments(left, bottom, left, top)
  segments(right, bottom, right, top)
  text(.5*(left+right), .5*(top+bottom), labels=j, cex=4, col="red")
})
```

```{r, echo=FALSE}
plot(tree)
text(tree, pretty=0)
```

Regression trees are built in R using a similar interface as linear models

```{r, eval=FALSE}
library(tree)
library(ISLR)
data(Auto)

tree_fit <- tree(mpg~horsepower+weight, data=Auto)
predict(tree_fit)
```

### Specifics of the regression tree algorithm

The recursive partitioning algorithm described above leads to a set of natural questions:

_When do we stop partitioning?_ We stop when adding a partition does not reduce RSS, or, when partition has too few training observations. Even then, trees built with this stopping criterion tend to _overfit_ training data. To avoid this, a post-processing step called _pruning_ is used to make the tree smaller.

**Question:** why would a smaller tree tend to generalize better?

Chapter 8 on the ISLR book, and your ML class will go into specifics of how to prune regression trees. Let's compare however, how do regression trees of different depths perform on both training and testing data.


```{r, echo=TRUE}
set.seed(1234)
train_indices <- sample(nrow(Auto), nrow(Auto)/2)
train_set <- Auto[train_indices,]
test_set <- Auto[-train_indices,]

auto_tree <- tree(mpg~cylinders+displacement+horsepower+weight+acceleration+year+factor(origin), data=train_set)
plot(auto_tree)
text(auto_tree, pretty=0, cex=1.4)
```

The `cv.tree` function is used to determine a reasonable tree depth for the given dataset. For this dataset it seems that a depth of 5 works well.

```{r, echo=TRUE}
cv_auto <- cv.tree(auto_tree)
plot(cv_auto$size, cv_auto$dev, type="b", xlab="Tree Size", ylab="RSS")
```

Here we see how deep trees can overfit this dataset and perform poorly on new, unseen data.

```{r, echo=TRUE}
plot(train_set$mpg, predict(auto_tree, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="Full Tree Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(auto_tree, newdata=train_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

Here is the tree we think will do a good job on prediction based on the plot above.

```{r, echo=TRUE}
pruned_auto <- prune.tree(auto_tree, best=5)
plot(train_set$mpg, predict(pruned_auto, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="Pruned Tree Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(pruned_auto, newdata=train_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

```{r, echo=FALSE}
plot(test_set$mpg, predict(pruned_auto, newdata=test_set), xlab="Observed MPG", ylab="Predicted MPG", main="Pruned Tree Testing Error")
abline(0,1)

rmse <- sqrt( mean( (test_set$mpg - predict(pruned_auto, newdata=test_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

### Classification (Decision) Trees

Classification, or decision trees, are used in classification problems, where the outcome is categorical. The same partitioning principle, but now, each region predicts the majority class for training observations within region. The recursive partitioning algorithm we saw previosuly requires a score function to choose predictors (and values) to partition with. In classification we could use a naive approach of looking for partitions that minimize training error. However, better performing approaches use more sophisticated metrics. Here are two of the most popular (denoted for leaf $m$):
  - **Gini Index**: $\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$, or
  - **Entropy**: $-\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$
  
where $\hat{p}_{mk}$ is the proportion of training observations in partition $m$ labeled as class $k$. Both of these seek to partition observations into subsets that have the same labels.

Let's look at how a classification tree performs on the credit card default dataset we saw before.

```{r, echo=TRUE}
data(Default)

with(Default, {
     plot(balance, income, pch=ifelse(student=="Yes", 19, 21), col=default)
     legend("topright", pch=c(19,21,19,19), col=c("black","black",1,2), legend=c("Student", "Not Student","Not Default","Default"))
})
```

```{r, echo=TRUE}
default_tree <- tree(default~student+balance+income, data=Default)
plot(default_tree)
text(default_tree, pretty=0)
```

```{r, echo=TRUE}
default_tree
```

Classification trees have certain advantages that make them very useful. They are
highly interpretable, even moreso than linear models. Are easy to visualize (if small enough), they (maybe) model human decision processes and don't require that dummy predictors for categorical variables are used.

On the other hand, the greedy approach via recursive partitioning is a bit harder to train than linear regression. It may not always be the best performing method since it is not very flexible and are highly unstable to changes in training data.

### Random Forests

Random Forests are a **very popular** approach that addresses these shortcomings via resampling of the training data. Their goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness). It uses two tricks to accomplish this.

The first trick is *Bagging* (bootstrap aggregation)
General scheme:
  1. Build many decision trees $T_1, T_2, \ldots, T_B$ from training set
  2. Given a new observation, let each $T_j$ predict $\hat{y}_j$
  3. For regression: predict average $\frac{1}{B} \sum_{j=1}^B \hat{y}_j$,
     for classification: predict with majority vote (most frequent class)
     
But wait, how do we get many decision trees from a single training set?

For this we use a clever resampling technique called the _bootstrap_. To create $T_j, \, j=1,\ldots,B$ from training set of size $n$:

a) create a bootstrap training set by sampling $n$ observations from training set **with replacement**
b) build a decision tree from bootstrap training set

![](bootstrap.png)

The second trick used in Random Forests is to use a random selection of features to split when deciding partitions. Specifically, when building each tree $T_j$, at each recursive partition only consider a randomly selected subset of predictors to check for best split. This reduces correlation between trees in forest, improving prediction accuracy.

Let's look at our auto dataset again

```{r, echo=TRUE}
set.seed(1234)
train_indices <- sample(nrow(Auto), nrow(Auto)/2)
train_set <- Auto[train_indices,]
test_set <- Auto[-train_indices,]

library(randomForest)

auto_rf <- randomForest(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, importance=TRUE, mtry=3, data=train_set)
```

Let's plot the predicted miles per gallon given by this model compared to the observed miiles per gallon in the training dataset.

```{r}
plot(train_set$mpg, predict(auto_rf, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="RF Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(auto_rf, newdata=train_set) )^2 ))

legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

Now let's look at the same plot on a _testing_ dataset.

```{r, echo=TRUE}
plot(test_set$mpg, predict(auto_rf, newdata=test_set), xlab="Observed MPG", ylab="Predicted MPG", main="RF Testing Error")
abline(0,1)

rmse <- sqrt( mean( (test_set$mpg - predict(auto_rf, newdata=test_set) )^2 ))

legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

A disadvantage of random forests is that we lose interpretability. However, we can use the fact that a bootstrap sample was used to construct trees to measure _variable importance_ from the random forest.

Here is a table of _variable importance_ for the random forest we just constructed.

```{r, echo=TRUE, results="asis"}
variable_importance <- importance(auto_rf)
knitr::kable(head(round(variable_importance, digits=2)))
```

And a barplot of the same data.

```{r, echo=FALSE}
imp <- importance(auto_rf)[,2]
par(mar=par()$mar+c(0,5,0,0))
o <- order(imp)
barplot(imp[o], horiz=TRUE, xlab="Variable Importance", las=2, cex.names=1.6)
```

### Tree-based methods summary

Tree-based methods are very interpretable _prediction_ models. For which some inferential tasks are possible (e.g., variable importance in random forests), but are much more limited than the linear models we saw previously. These methods are very commonly used across many application domains and Random Forests often perform at state-of-the-art for many tasks.

