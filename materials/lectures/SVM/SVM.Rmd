---
title: Support Vector Machine
author: CMSC320
date: "`r Sys.Date()`"
output: html_document
---

Support Vector Machines are state-of-the-art classification methods.
It is a flexible and efficient framework to learn classifers. They build
upon linear methods we have discussed previously and have a nice geometric interpretation of how they are trained (based maximum margin arguments). Their flexibility comes from the fact
that they can be trained over _similarities_ between observations (more on this later) rather than standard data in tabular form. This is useful in applications where string similarities, or network similarities are readily available. SVMs also follow the "classification as a space partition" framework that we have seen for logistic regression and decision trees.

```{r, echo=FALSE}
library(MASS)

library(RColorBrewer)
mycols <- brewer.pal(8, "Dark2")[c(3,2)]

s <- sqrt(1/5)
set.seed(30)

makeX <- function(M, n=100, sigma=diag(2)*s) {
  z <- sample(1:nrow(M), n, replace=TRUE)
  m <- M[z,]
  return(t(apply(m,1,function(mu) mvrnorm(1,mu,sigma))))
}

M0 <- mvrnorm(10, c(1,0), diag(2)) # generate 10 means
x0 <- makeX(M0) ## the final values for y0=blue

M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- makeX(M1)

x <- rbind(x0, x1)
y <- c(rep(0,100), rep(1,100))
cols <- mycols[y+1]

GS <- 75 # put data in a Gs x Gs grid
XLIM <- range(x[,1])
tmpx <- seq(XLIM[1], XLIM[2], len=GS)

YLIM <- range(x[,2])
tmpy <- seq(YLIM[1], YLIM[2], len=GS)

newx <- expand.grid(tmpx, tmpy)
colnames(newx) <- c("X1","X2")
```

```{r, echo=FALSE, fig.height=10, fig.width=10}
layout(matrix(1:4, nr=2, byrow=TRUE))
plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n", main="Training Set")
points(x, col=cols)

# linear SVM
library(e1071)
dat <- data.frame(X1=x[,1], X2=x[,2])
fit <- svm(y~X1+X2, data=dat, cost=1, kernel="linear", type="C-classification")
yhat <- attr(predict(fit, newdata=newx, decision.values=TRUE), "decision.values")[,1]
yhat <- ifelse(yhat > 0, 2, 1)
colshat <- mycols[yhat]

plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="linear svm")
points(x, col=cols)
points(newx, col=colshat, pch=".")

contour(tmpx, tmpy, matrix(as.numeric(yhat),GS,GS), levels=c(1,2), add=TRUE, drawlabels=FALSE)

fit <- svm(y~X1+X2, data=dat, cost=1, kernel="radial", type="C-classification", gamma=1)
yhat <- attr(predict(fit, newdata=newx, decision.values=TRUE), "decision.values")[,1]
yhat <- ifelse(yhat > 0, 2, 1)
colshat <- mycols[yhat]

plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="non-linear svm RBF gamma=1")
points(x, col=cols)
points(newx, col=colshat, pch=".")

contour(tmpx, tmpy, matrix(as.numeric(yhat),GS,GS), levels=c(1,2), add=TRUE, drawlabels=FALSE)

fit <- svm(y~X1+X2, data=dat, cost=1, kernel="radial", type="C-classification", gamma=5)
yhat <- attr(predict(fit, newdata=newx, decision.values=TRUE), "decision.values")[,1]
yhat <- ifelse(yhat > 0, 2, 1)
colshat <- mycols[yhat]

plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="non-linear svm RBF gamma=5")
points(x, col=cols)
points(newx, col=colshat, pch=".")

contour(tmpx, tmpy, matrix(as.numeric(yhat),GS,GS), levels=c(1,2), add=TRUE, drawlabels=FALSE)
```

### The two-class linear Support Vector Machine

Given training data: $\{(\mathbf{x}_1,y_1), (\mathbf{x}_2,y_2),\ldots,(\mathbf{x}_n,y_n)\}$,
where $\mathbf{x}_i$ is a vector of $p$ predictor values for $i$th observation, and
$y_i$ is the class label (we're going to use +1 and -1), SVMs define
a _discriminative_ function such that

$$
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} > 0 \, \mathrm{ if } y_i = 1
$$

and

$$
\beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} < 0 \, \mathrm{ if } y_i = -1
$$

Note that points where the _discriminative_ function equals 0 form a _hyper-plane_ (i.e., a line in 2D)

![](9_2.png)

A central concept in SVMs that we did not see in logistic regression is **the margin**: the distance between the separating plane and its nearest datapoints.

```{r, echo=FALSE}
library(png)
library(grid)

img <- readPNG("9_3.png")
grid.raster(img)
```

The SVMs is built from tree _key insights_:

1. **Look for the maximum margin hyper-plane**
2. Only depends on a subset of observations (support vectors)
3. Only depends on pair-wise "similarity" functions of observations

Let's see these in turn:

**Look for the maximum margin hyper-plane**

The goal is to find the plane (think line in 2D) that separates training data with largest margin. This will tend to _generalize_ better since new observations have room to fall within margin and still be classified correctly. This can be cast as _optimization_ problem (see _Numerical Methods_ or _Machine Learning_ class for details):

$$
\mathrm{max}_{\beta_0,\beta_1,\ldots,\beta_p} M \\
\mathrm{s.t} \sum_{j=1}^p \beta_p^2 = 1 \\
y_i(\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}) \geq M \, \forall i
$$

This runs however into a complication: _What if there is no separating hyper-plane?_
The solution is to penalize observations on the **wrong side of the margin**.

![](9_6.png)

$$
\mathrm{max}_{\beta_0,\beta_1,\ldots,\beta_p} M \\
\mathrm{s.t} \sum_{j=1}^p \beta_p^2 = 1 \\
y_i(\beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip}) \geq M(1-\epsilon_i) \, \forall i \\
\epsilon_i \geq 0 \, \forall i \\
\sum_{i=1}^n \epsilon_i \leq C
$$

$C$ is a parameter that tradeoffs the width of the margin vs. the penalty on observations on the _wrong_ side of the margin. This parameter $C$ has to be selected by the user or via cross-validation model selection methods we saw before.

```{r, echo=FALSE, fig.width=10, fig.height=10}
img <- readPNG("9_7.png")
grid.raster(img)
```


_Key insight no. 2_: **SVMs only depend on a subset of observations (support vectors)**

As a result of maximum-margin formulation, we only need observations that are on the "wrong" side of the margin to get $\beta$ values. These are called _support vectors_. In general: the smaller the parameter is $C$, the learned SVM will have fewer SVs. You can also think of the number of SVs as a rough measure of the _complexity_ of the SVM obtained. 

_Key insight no. 3_: **SVMs only depend on pairwise "similarity" functions of observations**

We can solve the optimization problem above only using inner products between observations (as opposed to the observations themselves)

_Inner product_: $\langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij}x_{i'j}$

As a result, we can write the _discriminant_ function in equivalent form

$$
f(x) = \beta_0 + \sum_{i=1}^n \alpha_i \langle x, x_i \rangle
$$

which, by definition, has $\alpha_i > 0$ **only** for SVs

### Non-linear Support Vector Machine

This last insight is how learn non-linear discriminative functions in SVMs. We can generalize inner product using "kernel" functions that provide something like an inner product:

$$
f(x) = \beta_0 + \sum_{i=1}^n \alpha_i k(x, x_i)
$$


![](9_8.png)


But, what is $k$? Let's consider two examples.

- _Polynomial kernel_: $k(x,x_i) = 1+\langle x, x_i \rangle^d$

- _RBF (radial) kernel_: $k(x,x_i) = \exp\{-\gamma \sum_{j=1}^p (x_{j}-x_{ij})^2\}$

![](9_9.png)

```{r, echo=FALSE}
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))

k <- function(x, x0=0, gamma=1) {
  exp(-gamma*(x-x0)^2)
}
x <- seq(-3, 3, len=100)
plot(x, k(x), type="l", lwd=2, col=1, main="RBF kernel")
lines(x, k(x,gamma=10), lwd=2, col=2)
lines(x, k(x,gamma=.1), lwd=2, col=3)
legend("topright", legend=paste("gamma=",c(1,10,.1)), lty=1, lwd=2, col=1:3)
```

## Fitting Support Vector Machines in R

Again, the familiar _formula_ interface is used to train SVMs. In this case we indicate that we are learning a _linear_ SVM using the `kernel` function argument. The tradeoff parameter $C$ is indicated in the `cost` function argument. Here we are fitting three different SVMs resulting from using three different values of $C$.

```{r}
library(e1071)
library(ISLR)
data(Default)

n <- nrow(Default)
train_indices <- sample(n, n/2)

costs <- c(.01, 1, 100)
svm_fits <- lapply(costs, function(cost) {
  svm(default~., data=Default, cost=cost, kernel="linear",subset=train_indices)
})
```


Let's take a look at how these SVMs behave:

```{r}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=costs, number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab)
```

Let's try now a _non-linear_ SVM by using a radial kernel and indicating that using the
`kernel` function argument. Notice now that we have two parameters to provide to the fitting function: tradeoff parameter $C$ and parameter $\gamma$ of the radial kernel function.

```{r}
costs <- c(.01, 1, 10)
gamma <- c(.01, 1, 10)
parameters <- expand.grid(costs, gamma)

svm_fits <- lapply(seq(nrow(parameters)), function(i) {
  svm(default~., data=Default, cost=parameters[i,1], kernel="radial", gamma=parameters[i,2], subset=train_indices)
})
```


Let's take at the result in this case:

```{r}
number_svs <- sapply(svm_fits, function(fit) fit$tot.nSV)
error_rate <- sapply(svm_fits, function(fit) {
  yhat <- predict(fit, newdata=Default[train_indices,])
  train <- mean(yhat != Default$default[train_indices])
  yhat <- predict(fit, newdata=Default[-train_indices,])
  test <- mean(yhat != Default$default[-train_indices])
  c(train=train, test=test)
})

tab <- data.frame(cost=parameters[,1], gamma=parameters[,2], number_svs=number_svs, train_error=error_rate["train",]*100,test_error=error_rate["test",]*100)
knitr::kable(tab)
```

# K-nearest neighbors

K-nn is a  related approach to obtain non-linear classification boundaries. It is a "memory" method as it requires the entire training data is used to make predictions on new data. Conceptually, the classifier is very simple: to make a prediction on entity $x$:

- Find $k$ nearest observations in training set (note this requires a distance function, e.g., Euclidean distance)
- Predict the majority class within the $k$ nearest neighbors

The `class::knn` function can be used in R to use this classifier.

Note that the number of neighbors $k$ is a hyper-parameter that must be selected before making predictions. Selecting $k$ falls under the _model selection_ problem we will discuss later.

